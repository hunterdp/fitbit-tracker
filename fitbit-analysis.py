# /usr/bin/python3
# -*- coding: utf-8 -*-
# MIT License

# Copyright (c) 2019 David Hunter

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

""" Generates analytics for data generated by fitbit-tracker.py

Reads in fitbit data generated by the fitbit-tracker.py program and generates
a series of data insights, graphs, charts and/or tables.

"""
# TODO(dph): Think about ading a configuration file as the options get more complicated
#            with regards to statistics
# TODO(dph): Add progess notification to the user.  This helps with large data sets.
# TODO(dph): Add a DEBUG or INFO function that will only execute debug and info msgs

from datetime import timedelta
from datetime import date
from datetime import datetime
from tqdm import tqdm

import argparse
import io
import os
import os.path
import sys
import logging
import logging.handlers
import json

import pandas as pd
import numpy as numpy
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.dates as md
import seaborn as sns
import pandas_profiling
import statsmodels.api as sm
import statsmodels.formula.api as smf

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()


# Globals
__AUTHOR__ = 'David Hunter'
__VERSION__ = 'beta-0.9'
__LOG_NAME__ = 'fitbit-analysis.log'
__TITLE__ = 'fitebit-analysis.py'
__DEBUG__ = False

def set_command_options():
    """ Defines command line arguments."""
    usage = 'Analyze Fitbit data generated by the fitbit-tracker.py program.'
    parser = argparse.ArgumentParser(prog='Fitbit Analysis', description=usage,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    type_group = parser.add_mutually_exclusive_group(required=True)
    date_group = parser.add_mutually_exclusive_group(required=True)
    parser.add_argument('--log', '--log_level',
                        help='Set the logging level [debug info warn error] (default: %(default)s)',
                        action='store', dest='log_level', type=str, default='info')
    parser.add_argument('-l', '--log_file', help='Set the logfile name. (default: %(default)s)',
                        action='store', type=str, default='fitbit-tracker.log')
    parser.add_argument('-o', '--output',  help='Output directory to store results files. (default: %(default)s)',
                        action='store', type=str,  dest='output_dir', default='results')
    parser.add_argument('-v', '--version', help='Prints the version',
                        action='version', version=__VERSION__)
    parser.add_argument('-r', '--retain_files',  help='Save consolidate dataframes',
                        dest='retain_dfs', type=bool)
    parser.add_argument('-e', '--end_date',  help='End date to analyze data from (yyyy-mm-dd)',
                        action='store', type=str,  dest='end_date')
    msg = 'Set the types of statistics to calculate for the collected data.  The options are all, min, middle or max.'
    parser.add_argument('--stats', help=msg, action='store',
                        dest='stats', type=str, default='min')
    msg = 'Interpolate missing heartrate data based on surrounding values in the same day.'
    parser.add_argument('-i', '--interpolate', help=msg, action='store_true')
    type_group.add_argument(
        '-a', '--all', help='Analyze all the data possible', action='store_true')
    type_group.add_argument('-t', '--type', help='Analyze only the type of data specified (heartrate, sleep, steps)',
                            action='store', type=str, dest='collect_type')
    date_group.add_argument('-s', '--start_date',  help='Start date toanalyze data from (yyyy-mm-dd)',
                            action='store', type=str,  dest='start_date')
    date_group.add_argument('--days', help='Number of days to go back and analyze',
                            action='store', type=int, dest='number_of_days')
    date_group.add_argument('--date', help='Specifc date to collect for',
                            action='store', type=str, dest='date_to_collect')
    return(parser)


def get_command_options(parser):
    """ Retrieves the command line options and returns a kv dict """
    global __DEBUG__
    args = parser.parse_args()
    fmt = "%(asctime)-15s %(levelname)-8s %(lineno)5d:%(module)s:%(funcName)-25s %(message)s"
    log_file = args.log_file
    options = {'log_file': args.log_file}
    line_sep = '--------------------------------------------------------------------------------'
    msg = 'Starting ' + __TITLE__ + ' ' + __VERSION__

    # Set up the logging infrastructure before we do anything.  To ensure we mark the start of a
    # new instance, log the initialization using the level chosen.
    if args.log_level:
        if 'debug' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.DEBUG)
            logging.debug(line_sep)
            logging.debug(msg)
            logging.debug(line_sep)
            options['log_level'] = 'debug'
            __DEBUG__ = True

        elif 'warn' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.WARNING)
            logging.warning(line_sep)
            logging.warning(msg)
            logging.warning(line_sep)
            options['log_level'] = 'warn'

        elif 'error' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.ERROR)
            logging.error(line_sep)
            logging.error(msg)
            logging.error(line_sep)
            options['log_level'] = 'error'

        elif 'info' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.INFO)
            logging.info(line_sep)
            logging.info(msg)
            logging.info(line_sep)
            options['log_level'] = 'info'

        else:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.INFO)
            logging.error('Invalid debug level.  Exiting the program.')
            sys.exit(1)

    # Ensure something is specified to be analyzed
    if args.all:
        options['analyze_type'] = 'steps heartrate sleep'
    elif args.collect_type:
        options['analyze_type'] = args.collect_type
        logging.info('Analyzing: ' + args.collect_type)
    else:
        msg = 'Specify the type of data to analyze or use the -a flag'
        logging.error(msg)
        print('Specify the type of data to analyze or use the -a flag')
        sys.exit(1)

    # What is the time period
    if ((args.number_of_days and (args.start_date or args.end_date))
            or (args.date_to_collect and (args.start_date or args.end_date))):
        logging.error('Illegal date specifications.  Exiting')
        sys.exit(1)
    elif args.number_of_days:
        if args.number_of_days <= 0:
            msg = 'Number of days needs to be greater than zero.  Exiting'
            logging.error(msg)
            print(msg)
            sys.exit(1)
        else:
            options['number_of_days'] = args.number_of_days
            logging.info("Number of days previous: " +
                         str(options['number_of_days']))
    elif args.date_to_collect:
        # Collect for a specific day
        if is_valid_date(args.date_to_collect):
            options['date_to_collect'] = args.date_to_collect
            logging.info("Date to collect for: " +
                         str(options['date_to_collect']))
        else:
            print('Invalid date specified.  Exiting.')
            logging.error('Invalid date: ' + str(args.date_to_collect))
            sys.exit(1)
    elif args.start_date and args.end_date:
        if not is_valid_date(args.start_date):
            print('Invalid start date specified.  Exiting.')
            logging.error('Invalid start date: ' + str(args.start_date))
            sys.exit(1)
        elif not is_valid_date(args.end_date):
            print('Invalid end date specified.  Exiting.')
            logging.error('Invalid end date: ' + str(args.end_date))
            sys.exit(1)
        else:
            options['start_date'] = args.start_date
            options['end_date'] = args.end_date
            logging.info('Start date: ' + args.start_date)
            logging.info('End date: ' + args.end_date)
            if args.start_date > args.end_date:
                msg = 'Start date is after end date. Exiting.'
                print(msg)
                logging.error(msg)
                sys.exit(1)
    else:
        # Start and end date not specified.
        msg = 'Both start and end dates need to be specified. Exiting.'
        logging.error(msg)
        print(msg)
        sys.exit(1)

    # Ensure ouput directory is valid
    if not os.path.isdir(args.output_dir):
        msg = 'Directory does not exist.  Exiting.'
        logging.error(msg)
        print(msg)
        sys.exit(1)
    else:
        options['output_dir'] = args.output_dir

    # Use interpolation.  Only viable for heartrate.
    if args.interpolate and 'heartrate' in args.collect_type:
        options['interpolate'] = True
        msg = 'Using interpolation for heartrate.'
        logging.info(msg)
    else:
        options['interpolate'] = False
        msg = 'Do not use interpolation.'
        logging.info(msg)

    if args.retain_dfs:
        options['retain'] = True
    else:
        options['retain'] = False

    logging.debug(json.dumps(options))
    return(options)


def is_valid_date(date_to_check):
    """ Checks to see if the date is valid """
    year, month, day = date_to_check.split('-', 3)
    try:
        date(int(year), int(month), int(day))
    except ValueError:
        return(False)
    return(True)


def get_dataframe(fname):
    """ Reads in a file generated by fitbit-tracker, converts the index into a timedelta value
        and returns the results in a dataframe """
    df = pd.read_csv(fname, sep=',', header=0, index_col=0,
                     skip_blank_lines=True, dtype={1: 'int32'})
    df.index = pd.TimedeltaIndex(df.index)
    # df.index = pd.DatetimeIndex(df.index)
    return(df)


def get_all_file_list(dir_name, fragment):
    """ Gets a list of files within a directory based on a string in the filename """
    if os.path.isdir(dir_name):
        all_files = []
        list_of_files = os.listdir(dir_name)
        for file_name in list_of_files:
            if fragment in file_name:
                full_path_name = os.path.join(dir_name, file_name)
                all_files.append(full_path_name)
        return(all_files)
    else:
        logging.error(
            'Invalid directory provided in get_all_file_list function.  Exiting')
        exit(1)

def date_range(start, end):
    """ Returns a list of dates """
    r = (end+timedelta(days=1)-start).days
    return[start+timedelta(days=i) for i in range(r)]


def get_date_frag(options):
    """ Returns a list of file fragment using the dates specified in the options array """
    if 'end_date' in options and 'start_date' in options:
        start_date = datetime.strptime(options['start_date'], '%Y-%m-%d')
        end_date = datetime.strptime(options['end_date'], '%Y-%m-%d')
        number_of_days_requested = (end_date - start_date).days
        logging.info('Startdate:' + str(start_date))
        logging.info('Enddate:' + str(end_date))
        logging.info('Days requested vale: ' + str(number_of_days_requested))
        date_list = date_range(start_date, end_date)

    elif 'number_of_days' in options:
        # Use the --days option
        number_of_days_requested = 1
        today = datetime.today()
        # TODO(dph): The line below throws the "FutureWarning: Addition/subtraction of
        # integers and integer-arrays to DatetimeArray is deprecated, will be removed
        # in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`
        start_date = today - timedelta(days=options['number_of_days'])
        logging.info('Collect data for: ' + str(start_date))
        date_list = date_range(start_date, start_date)

    elif 'date_to_collect' in options:
        number_of_days_requested = 1
        start_date = datetime.strptime(options['date_to_collect'], '%Y-%m-%d')
        logging.info('Collect for the specific date:' + options['date_to_collect'])
        date_list = date_range(start_date, start_date)
    else:
        logging.error('No date specified.  Exiting')
        sys.exit(1)

    date_frag_list = list()
    for date in date_list:
        date_frag_list.append(datetime.strftime(date, '%Y-%m-%d'))

    return(date_frag_list)


def create_index_file(fname, start, end, freq):
    """ Creates an dataframe with an time index and stores it in the passed filename """
    # TODO(dph): Look at changing this to a TimeDeltaIndex or a DatetimeIndex model
    rng = pd.date_range(start=start, end=end, freq=freq)
    base_df = pd.DataFrame(data={'Time': rng.strftime('%H:%M:%S')})
    base_df.to_csv(fname, columns=['Time'], header=True, index=False)


def generate_stats_df(df, axis):
    """ Given a dataframe, return a dataframe with stats along the given axis"""
    stats_df = pd.DataFrame()
    stats_df['Mean'] = df.mean(axis=axis)
    stats_df['Median'] = df.median(axis=axis)
    stats_df['Min'] = df.min(axis=axis)
    stats_df['Max'] = df.max(axis=axis)
    stats_df['MinIdx'] = df.idxmin(axis=axis)
    stats_df['MaxIdx'] = df.idxmax(axis=axis)
    stats_df['StdDev'] = df.std(axis=axis)
    return stats_df


def get_sum_of_axis(df, axis):
    """ Returns a series of the summary of the given axis """
    sum = df.sum(axis=axis, skipna=True)
    return(sum)


def log_debug_list(_list, prog_msg, log_msg):
    """ Given a list, log it to the debug log file """
    logging.debug(log_msg)
    prog_bar = tqdm(total=len(_list), desc=prog_msg, ascii=True)
    for i in _list:
        logging.debug('\t' + i)
        prog_bar.update()


if __name__ == '__main__':
    parser = set_command_options()
    options = get_command_options(parser)
    index_file = options['output_dir'] + '/intraday_index.csv'
    found_file_list = list()
    missing_file_list = list()

    # Create an index file that contains all timeslots in a day as the FitBit
    # sampling intervals can vary day to day.
    create_index_file(index_file, start='00:00:00', end='23:59:59', freq='S')
    merge_df = get_dataframe(index_file)
    merge_df.index = pd.TimedeltaIndex(merge_df.index)

    # Generate a list of all possible filenames during the requested time
    # period and create a list of valid files.
    frag_list = get_date_frag(options)
    prog_bar = tqdm(total=len(frag_list), desc='Creating file list', ascii=True)
    for frag in frag_list:
        if 'heartrate' in options['analyze_type']:
            f1 = options['output_dir'] + '/hr_intraday_' + str(frag) + '.csv'
        elif 'steps' in options['analyze_type']:
            f1 = options['output_dir'] + \
                '/steps_intraday_' + str(frag) + '.csv'
        elif 'sleep' in options['analyze_type']:
            f1 = options['output_dir'] + '/sleep_day_' + str(frag) + '.csv'
        if os.path.exists(f1):
            found_file_list.append(f1)
        else:
            missing_file_list.append(f1)
        prog_bar.update()
    logging.info('Looked for ' +str(len(frag_list)) + ' files.')
    logging.info('Found ' + str(len(found_file_list)) + ' files.')
    logging.info('Missing  ' + str(len(missing_file_list)) + ' files.')

    if __DEBUG__:
        log_debug_list(frag_list,'Writing found fragment list','Fragment Files:')
        log_debug_list(found_file_list,'Writing found file list','Found Files:')
        log_debug_list(missing_file_list, 'Writing missing file list','Missing Files:')

    if len(found_file_list) == 0:
        msg = 'No matching files find for request.'
        print(msg)
        logging.error(msg)
        exit(-1)

    # Merge the files into a single dataframe, keeping track of what has/has not merged.
    merged_file_list = list()
    empty_file_list = list()
    all_zeros_file_list = list()
    prog_bar = tqdm(total=len(found_file_list), desc='Merging Files', ascii=True)
    for fname in found_file_list:
        df = get_dataframe(fname)
        # if the max and min are 0 consider the dataframe empty.
        for cols in df.columns:
            df_min = df[cols].min()
            df_max = df[cols].max()
        if df.empty:
            empty_file_list.append(fname)
        elif df_min == 0 and df_max == 0:
            all_zeros_file_list.append(fname)
        else:
            merge_df = pd.merge(merge_df, df, left_index=True,
                                right_index=True, how='left')
            merged_file_list.append(fname)
        prog_bar.update()
    logging.info('Merged ' + str(len(merged_file_list)) + ' files.')
    logging.info(str(len(empty_file_list)) + ' files not merged:')
    logging.info(str(len(all_zeros_file_list)) + ' files with all zeros.')

    if __DEBUG__:
        log_debug_list(merged_file_list,'Writing merged file list','Merged Files: ')
        log_debug_list(empty_file_list, 'Writing empty file list', 'Empty Files: ')
        log_debug_list(all_zeros_file_list, 'Writing list of all zero files','Files with all zeros: ')

    if options['interpolate']:
        # Within the heartrate dataframe, NaaN values  will be interpolated  values based on
        # a linear algorithium.  While this may not be 100% accurate, it does assume that the
        # pace of change is realtively equal between sampled values.
        # Note:  If we interpolate along the column (axis=1) then we will get go along the lines of
        # assuming that the next value (fwd or bckwd) would be the logical value.  If we interpolate
        # along the row, aka: time based (axis=0), we assume that more often than not the missing
        # value is relatively constant across time.
        # TODO(dph): Explore the use of the limit_area, limit and limit_direction arguments to see if
        #            what the differences are.
        logging.info('Interpolating the merged dataframe values.')
        merge_df.interpolate(method='linear', axis=0, inplace=True, limit_direction='both')

    # if 'sleep' in options[anlyze_type]:
        # Fill any non-recorded time with 0, indicating awake except for the time
        # period when sleep started and ended.
        # sleep_start =
        # sleep_stop =

    # Create a summary dataframes for both the time and day axes
    time_summary_df = pd.DataFrame()
    day_summary_df = pd.DataFrame()

    # TODO(dph): This throws the warning "RuntimeWarning: All-NaN slice encountered overwrite_input=overwrite_input)"
    print('Generating basic statistics along the columns axis.')
    time_summary_df = generate_stats_df(merge_df, 'columns')
    print('Generating basic statistics along the index axis.')
    day_summary_df  = generate_stats_df(merge_df, 'index')

    if 'steps' in options['analyze_type']:
        day_summary_df['Total Steps'] = merge_df.sum(axis='index', skipna=True)

    column_name_list = list(merge_df.columns.values)
    index_names_list = list(merge_df.index.values)
    merge_df.hist()
    plt.show()

    # To do a simple analysis, add the sleep data into the merged dataframe to see what type of sleep was occuring during the heartrate

    # Retain summary files if requested
    if options['retain']:
        merge_df.to_csv('merged_df.csv')
        day_summary_df.to_csv('day_summary.csv')
        time_summary_df.to_csv('time_summary.csv')
