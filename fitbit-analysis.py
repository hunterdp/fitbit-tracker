# /usr/bin/python3
# -*- coding: utf-8 -*-
""" Generates analytics for data generated by fitbit-tracker.py

Reads in fitbit data generated by the fitbit-tracker.py program and generates
a series of data insights, graphs, charts and/or tables.

"""
# TODO(dph): Think about ading a configuration file as the options get more complicated
#            with regards to statistics
# TODO(dph): Add progess notification to the user.  This helps with large data sets.

from datetime import timedelta
from datetime import date
from datetime import datetime
from tqdm import tqdm

import argparse
import io
import os
import os.path
import sys
import logging
import logging.handlers
import json

import pandas as pd
import numpy as numpy
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.dates as md
import seaborn as sns
import pandas_profiling

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()


# Globals
__AUTHOR__ = 'David Hunter'
__VERSION__ = 'beta-0.9'
__LOG_NAME__ = 'fitbit-analysis.log'
__TITLE__ = 'fitebit-analysis.py'


def set_command_options():
    """ Defines command line arguments."""
    usage = 'Analyze Fitbit data generated by the fitbit-tracker.py program.'
    parser = argparse.ArgumentParser(prog='Fitbit Analysis', description=usage,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    type_group = parser.add_mutually_exclusive_group(required=True)
    date_group = parser.add_mutually_exclusive_group(required=True)
    parser.add_argument('--log', '--log_level',
                        help='Set the logging level [debug info warn error] (default: %(default)s)', action='store', dest='log_level', type=str, default='info')
    parser.add_argument('-l', '--log_file', help='Set the logfile name. (default: %(default)s)',
                        action='store', type=str, default='fitbit-tracker.log')
    parser.add_argument('-o', '--output',  help='Output directory to store results files. (default: %(default)s)',
                        action='store', type=str,  dest='output_dir', default='results')
    parser.add_argument('-v', '--version', help='Prints the version',
                        action='version', version=__VERSION__)
    parser.add_argument('-e', '--end_date',  help='End date to analyze data from (yyyy-mm-dd)',
                        action='store', type=str,  dest='end_date')
    msg = 'Set the types of statistics to calculate for the collected data.  The options are min, middle or max.  Note that max will take a long time.'
    parser.add_argument('--stats', help=msg, action='store',
                        dest='stats_to_calc', type=str, default='min')
    msg = 'Interpolate missing data based on surrounding values in the same day.'
    parser.add_argument('--interpolate', help=msg, action='store',
                        dest='interp_data', type=bool, default='no')
    type_group.add_argument(
        '-a', '--all', help='Analyze all the data possible', action='store_true')
    type_group.add_argument('-t', '--type', help='Analyze only the type of data specified (heartrate, sleep, steps)',
                            action='store', type=str, dest='collect_type')
    date_group.add_argument('-s', '--start_date',  help='Start date toanalyze data from (yyyy-mm-dd)',
                            action='store', type=str,  dest='start_date')
    date_group.add_argument('--days', help='Number of days to go back and analyze',
                            action='store', type=int, dest='number_of_days')
    date_group.add_argument('--date', help='Specifc date to collect for',
                            action='store', type=str, dest='date_to_collect')
    return(parser)


def get_command_options(parser):
    """ Retrieves the command line options and returns a kv dict """
    args = parser.parse_args()
    fmt = "%(asctime)-15s %(levelname)-8s %(lineno)5d:%(module)s:%(funcName)-25s %(message)s"
    log_file = args.log_file
    options = {'log_file': args.log_file}
    line_sep = '--------------------------------------------------------------------------------'
    msg = 'Starting ' + __TITLE__ + ' ' + __VERSION__

    # Set up the logging infrastructure before we do anything.  To ensure we mark the start of a
    # new instance, log the initialization using the critical level.
    if args.log_level:
        if 'debug' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.DEBUG)
            logging.debug(line_sep)
            logging.debug(msg)
            logging.debug(line_sep)

        elif 'warn' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.WARNING)
            logging.warn(line_sep)
            logging.warn(msg)
            logging.warn(line_sep)

        elif 'error' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.ERROR)
            logging.error(line_sep)
            logging.error(msg)
            logging.error(line_sep)
        elif 'info' in args.log_level:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.INFO)
            logging.info(line_sep)
            logging.info(msg)
            logging.info(line_sep)
        else:
            logging.basicConfig(filename=args.log_file,
                                format=fmt, level=logging.INFO)
            logging.error('Invalid debug level.  Exiting the program.')
            sys.exit(1)

    if args.all:
        options['analyze_type'] = 'steps heartrate sleep'
    elif args.collect_type:
        options['analyze_type'] = args.collect_type
        logging.info('Analyzing: ' + args.collect_type)
    else:
        logging.error(
            'You need to specify the type of data to analyze or use the -a flag')
        sys.exit(1)

    if ((args.number_of_days and (args.start_date or args.end_date))
            or (args.date_to_collect and (args.start_date or args.end_date))):
        logging.error('Illegal date specifications.  Exiting')
        sys.exit(1)

    elif args.number_of_days:
        if args.number_of_days <= 0:
            logging.error(
                "Number of days needs to be greater than zero.  Exiting")
            sys.exit(1)
        else:
            options['number_of_days'] = args.number_of_days
            logging.info("Number of days previous: " +
                         str(options['number_of_days']))

    elif args.date_to_collect:
        # Collect for a specific day
        if is_valid_date(args.date_to_collect):
            options['date_to_collect'] = args.date_to_collect
            logging.info("Date to collect for: " +
                         str(options['date_to_collect']))
        else:
            print('Invalid date specified.  Exiting.')
            logging.error('Invalid date: ' + str(args.date_to_collect))
            sys.exit(1)

    elif args.start_date and args.end_date:
        if not is_valid_date(args.start_date):
            print('Invalid start date specified.  Exiting.')
            logging.error('Invalid start date: ' + str(args.start_date))
            sys.exit(1)
        elif not is_valid_date(args.end_date):
            print('Invalid end date specified.  Exiting.')
            logging.error('Invalid end date: ' + str(args.end_date))
            sys.exit(1)
        else:
            options['start_date'] = args.start_date
            options['end_date'] = args.end_date
            logging.info('Start date: ' + args.start_date)
            logging.info('End date: ' + args.end_date)
            if args.start_date > args.end_date:
                logging.error("Start date is after end date. Exiting.")
                sys.exit(1)

    else:
        # Start and end date not specified.
        logging.error(
            'Both start and end dates need to be specified. Exiting.')
        sys.exit(1)

    if not os.path.isdir(args.output_dir):
        logging.error('Directory does not exist')
        sys.exit(1)
    else:
        options['output_dir'] = args.output_dir

    logging.debug(json.dumps(options))
    return(options)


def is_valid_date(date_to_check):
    """ Checks to see if the date is valid """
    year, month, day = date_to_check.split('-', 3)
    try:
        date(int(year), int(month), int(day))
    except ValueError:
        return(False)
    return(True)


def get_dataframe(fname):
    """ Reads in a file generated by fitbit-tracker, converts the index into a timedelta value
        and returns the results in a dataframe """
    df = pd.read_csv(fname, sep=',', header=0, index_col=0,
                     skip_blank_lines=True, dtype={1: 'int32'})
    df.index = pd.TimedeltaIndex(df.index)
    # df.index = pd.DatetimeIndex(df.index)
    return(df)


def get_all_file_list(dir_name, fragment):
    """ Gets a list of files within a directory based on a string in the filename """
    if os.path.isdir(dir_name):
        all_files = []
        list_of_files = os.listdir(dir_name)
        for file_name in list_of_files:
            if fragment in file_name:
                full_path_name = os.path.join(dir_name, file_name)
                all_files.append(full_path_name)
        return(all_files)
    else:
        logging.error(
            'Invalid directory provided in get_all_file_list function.  Exiting')
        exit(1)


def date_range(start, end):
    """ Returns a list of dates """
    r = (end+timedelta(days=1)-start).days
    return[start+timedelta(days=i) for i in range(r)]


def get_date_frag(options):
    """ Returns a list of file fragment using the dates specified in the options array """
    if 'end_date' in options and 'start_date' in options:
        start_date = datetime.strptime(options['start_date'], '%Y-%m-%d')
        end_date = datetime.strptime(options['end_date'], '%Y-%m-%d')
        number_of_days_requested = (end_date - start_date).days
        logging.info('Startdate:' + str(start_date))
        logging.info('Enddate:' + str(end_date))
        logging.info('Days requested vale: ' + str(number_of_days_requested))
        date_list = date_range(start_date, end_date)

    elif 'number_of_days' in options:
        # Use the --days option
        today = datetime.today()
        # TODO(dph): The line below throws the "FutureWarning: Addition/subtraction of
        # integers and integer-arrays to DatetimeArray is deprecated, will be removed
        # in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`
        start_date = today - timedelta(days=options['number_of_days'])
        start_date_str = datetime.strftime(start_date, "%Y-%m-%d")
        number_of_days_requested = 1
        logging.info('Collect data for: ' + str(start_date))
        date_list = date_range(start_date, start_date)

    elif 'date_to_collect' in options:
        start_date = datetime.strptime(options['date_to_collect'], '%Y-%m-%d')
        start_date_str = options['date_to_collect']
        logging.info('Collect for the specific date:' + start_date_str)
        number_of_days_requested = 1
        date_list = date_range(start_date, start_date)
    else:
        logging.error('No date specified.  Exiting')
        sys.exit(1)

    date_frag_list = list()
    for date in date_list:
        date_frag_list.append(datetime.strftime(date, '%Y-%m-%d'))

    return(date_frag_list)


def create_index_file(fname, start, end, freq):
    """ Creates an dataframe with an time index and stores it in the passed filename """
    rng = pd.date_range(start=start, end=end, freq=freq)
    base_df = pd.DataFrame(data={'Time': rng.strftime('%H:%M:%S')})
    #base_df.to_csv(fname, columns=['Time'], header=True, index=False)
    # Writes out the index as well
    base_df.to_csv(fname, columns=['Time'], header=True, index=True)


def interpolate_gaps(values, limit=None):
    """ Fill gaps using linear interpolation, optionally only fill gaps up to a
    size of `limit`.
    """
    values = np.asarray(values)
    i = np.arange(values.size)
    valid = np.isfinite(values)
    filled = np.interp(i, i[valid], values[valid])

    if limit is not None:
        invalid = ~valid
        for n in range(1, limit+1):
            invalid[:-n] &= invalid[n:]
        filled[invalid] = np.nan

    return filled


def generate_stats_df(df, axis):
    """ Given a dataframe, return a dataframe with stats along the given axis"""
    stats_df = pd.DataFrame()
    stats_df['Mean'] = df.mean(axis=axis)
    stats_df['Median'] = df.median(axis=axis)
    stats_df['Min'] = df.min(axis=axis)
    stats_df['Max'] = df.max(axis=axis)
    stats_df['StdDev'] = df.std(axis=axis)
    return stats_df


if __name__ == '__main__':

    # Retrieve and valid command line options.
    parser = set_command_options()
    options = get_command_options(parser)
    index_file = options['output_dir'] + '/intraday_index.csv'

    # Generate the list of files to retrieve, keeping track of missing dates.
    frag_list = get_date_frag(options)
    file_list = list()
    missing_files = list()

    prog_bar = tqdm(total=len(frag_list), desc='Creating file list', ascii=True)
    for frag in frag_list:
        if 'heartrate' in options['analyze_type']:
            f1 = options['output_dir'] + '/hr_intraday_' + str(frag) + '.csv'
        elif 'steps' in options['analyze_type']:
            f1 = options['output_dir'] + \
                '/steps_intraday_' + str(frag) + '.csv'
        elif 'sleep' in options['analyze_dir']:
            f1 = options['output_dir'] + '/sleep_day_' + str(frag) + '.csv'
        if os.path.exists(f1):
            file_list.append(f1)
        else:
            missing_files.append(f1)
        prog_bar.update()

    # If no files match, just report and exit.
    if len(file_list) == 0:
        msg = 'No matching files find for request.'
        print(msg)
        logging.error(msg)
        exit(-1)

    logging.info('The number of files to look for was: ' +
                 str(len(frag_list)))
    logging.info('The number of files found is: ' +
                 str(len(file_list)))

    if len(file_list) != len(frag_list):
        for i in file_list:
            msg = 'Found file: ' + i
            logging.debug(msg)

        logging.info('Number of missing files is: ' +
                 str(len(missing_files)))
        for f in missing_files:
            msg = 'Missing file: ' + f
            logging.debug(msg)

    # Generate an index that consists of HH:MM:SS and convert it to the timedelta index.
    # This is used to ensure that all possible index values in the day can be captured
    # as the FitBit second intervals can vary day to day.  This ensures that no time slots
    # are not dropped when merged.
    #
    # Name the original index before setting the index to the timedelta.  The orginal index
    # is simplier and is equal to the secondcount in 24 hours (aka: 0 - 86399 => per day).
    # These will be swapped later on after the merge.
    create_index_file(index_file, start='00:00:00', end='23:59:59', freq='S')
    merge_df = pd.read_csv(index_file, sep=',', header=0, index_col=1,
                     skip_blank_lines=True)
    merge_df.columns=['Idx']
    merge_df.index = pd.TimedeltaIndex(merge_df.index)

    # Keep track of what has and has not been merged.
    # A df that has all 0s is considered empty.
    merged_file_list = list()
    empty_file_list = list()
    all_zeros_file_list = list()

    prog_bar = tqdm(total=len(file_list), desc='Merging Files', ascii=True)
    for fname in file_list:
        df = get_dataframe(fname)
        # There is only one column in the fitbit dataframe, if the max and min are 0
        # consider the dataframe empty.
        for cols in df.columns:
            df_min = df[cols].min()
            df_max = df[cols].max()
        if df.empty:
            empty_file_list.append(fname)
        elif df_min == 0 and df_max == 0:
            # empty_file_list.append(fname)
            all_zeros_file_list.append(fname)
        else:
            merge_df = pd.merge(merge_df, df, left_index=True,
                                right_index=True, how='left')
            merged_file_list.append(fname)
        prog_bar.update()

    logging.warning('Number of files merged:' + str(len(merged_file_list)))
    prog_bar = tqdm(total=len(merged_file_list), desc='Writing merged file list', ascii=True)
    for i in merged_file_list:
        logging.debug('Merged file: ' + i)
        prog_bar.update()

    logging.warning('Number of empty files not merged:' +
                    str(len(empty_file_list)))
    prog_bar = tqdm(total=len(empty_file_list), desc='Writing empty file list', ascii=True)
    for i in empty_file_list:
        logging.debug('Empty file, not merged: ' + i)
        prog_bar.update()

    logging.warning('Number of files with all zeros:' +
                    str(len(all_zeros_file_list)))
    prog_bar = tqdm(total=len(all_zeros_file_list), desc='Writing list of all zero files', ascii=True)
    for i in all_zeros_file_list:
        logging.debug('Zero filled file, not merged: ' + i)
        prog_bar.update()

    # To make indexing rows simplier, reset the index to a the second in the day vs timedelta value.  Note
    # that the original timedelta value is kept.  Note that we do not lose the timedelta column
    print('\nOriginal Merged Dataframe\n')
    print(merge_df.head(50))
    merge_df.plot()
    merge_df.set_index('Idx', inplace=True, drop=False)

#  print('Generating plot of merged dataframe.')
    merge_df.plot()

    # TODO(dph): Make this an option in cases where we want just the sampled values.
    # For NaaN values, we will interpolate their values based on a linear algorithium.  While this
    # may not be 100% accurate, it does assume that the pace of change is realtively equal between
    # sampled values.
    # Note:  If we interpolate along the column (axis=1) then we will get go along the lines of
    # assuming that the next value (fwd or bckwd) would be the logical value.  If we interpolate
    # along the row, aka: time based (axis=0), we assume that more often than not the missing
    # value is relatively constant across time.

    # TODO(dph): Explore the use of the limit_area, limit and limit_direction arguments to see if
    #            what the differences are.

    print('\nNew Index Merged Dataframe\n')
    print(merge_df.head(50))
    # time_summary_df = pd.DataFrame()
    # day_summary_df = pd.DataFrame()
    # time_summary_df = generate_stats_df(merge_df, 1)
    # day_summary_df  = generate_stats_df(merge_df, 0)
    # print('Time summary of original merged dataframe\n')
    # print('The total number of NaaN values is: ' + str(merge_df.isnull().sum().sum()))
    # print(time_summary_df)
    # print(day_summary_df)
    # columnList = list(merge_df.columns.values)
    # rowList = list(merge_df.index.values)
    # print(rowList)

    # print('Time summary of interpolated merged dataframe\n')
    # interp_time_summary_df = pd.DataFrame()
    # interp_day_summary_df = pd.DataFrame()
    # interp_merge_df = merge_df.copy(deep=True)
    # interp_merge_df.interpolate(method='linear', axis=0, inplace=True, limit_direction='both')
    # print('\nOriginal Interpolated Dataframe\n')
    # print(interp_merge_df.head(50))
    # interp_time_summary_df = generate_stats_df(interp_merge_df, 1)
    # interp_day_summary_df  = generate_stats_df(interp_merge_df, 0)
    # print('The total number of NaaN values is: ' + str(interp_merge_df.isnull().sum().sum()))
    # print(interp_time_summary_df)
    # print(interp_day_summary_df)

# Examine the difference between the interpolated and original


#  print('Generating plot of summary statistics based on time.')
#  time_summary_df.plot()
#  print('Generating plot of summary statistics based on day.')
#  day_summary_df.plot()

# Finally show the plots/charts.  Only call this at the end
    plt.show()

# To do a simple analysis, add the sleep data into the merged dataframe to see what type of sleep was occuring during the heartrate
